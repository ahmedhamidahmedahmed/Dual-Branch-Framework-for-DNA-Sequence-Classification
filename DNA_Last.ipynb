{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64004d7a-19af-4dc7-801a-e8430f768bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "GPU: NVIDIA GeForce RTX 3080\n",
      "DataLoader: num_workers=0, pin_memory=True, persistent=False\n",
      "Loading reference FASTA...\n",
      "Loading cache: df_model_500k.parquet\n",
      "Loaded cached variants: 191279\n",
      "Bad rows (non-ACGT): 0\n",
      "After filtering: 191279\n",
      "Sanity: fraction where seq_ref == seq_alt: 0.000110 (should be near 0)\n",
      "\n",
      "Fold 1: model device: cuda:0\n",
      "Fold 1: n_train=160,705, n_test=30,574, pos_frac=0.1869, focal_alpha=0.8131\n",
      "Fold 1 | Epoch 01 | loss=0.0789 | AUC=0.8118 | F1@0.5=0.4177 | F1@thr=0.5130 | thr=0.418 | pred_pos=0.162\n",
      "Fold 1 | Epoch 02 | loss=0.0680 | AUC=0.8305 | F1@0.5=0.4644 | F1@thr=0.5300 | thr=0.446 | pred_pos=0.151\n",
      "Fold 1 | Epoch 03 | loss=0.0633 | AUC=0.8349 | F1@0.5=0.4530 | F1@thr=0.5383 | thr=0.420 | pred_pos=0.149\n",
      "Fold 1 | Epoch 04 | loss=0.0599 | AUC=0.8406 | F1@0.5=0.5177 | F1@thr=0.5540 | thr=0.461 | pred_pos=0.160\n",
      "Fold 1 | Epoch 05 | loss=0.0573 | AUC=0.8430 | F1@0.5=0.5283 | F1@thr=0.5597 | thr=0.463 | pred_pos=0.161\n",
      "Fold 1 | Epoch 06 | loss=0.0546 | AUC=0.8439 | F1@0.5=0.5104 | F1@thr=0.5514 | thr=0.442 | pred_pos=0.153\n",
      "Fold 1 | Epoch 07 | loss=0.0527 | AUC=0.8394 | F1@0.5=0.4836 | F1@thr=0.5449 | thr=0.428 | pred_pos=0.155\n",
      "Fold 1 | Epoch 08 | loss=0.0509 | AUC=0.8412 | F1@0.5=0.5225 | F1@thr=0.5599 | thr=0.455 | pred_pos=0.153\n",
      "Fold 1 | Epoch 09 | loss=0.0490 | AUC=0.8459 | F1@0.5=0.5543 | F1@thr=0.5683 | thr=0.468 | pred_pos=0.163\n",
      "Fold 1 | Epoch 10 | loss=0.0473 | AUC=0.8415 | F1@0.5=0.5187 | F1@thr=0.5559 | thr=0.442 | pred_pos=0.151\n",
      "Fold 1 | Epoch 11 | loss=0.0459 | AUC=0.8416 | F1@0.5=0.5352 | F1@thr=0.5566 | thr=0.467 | pred_pos=0.151\n",
      "Fold 1 | Epoch 12 | loss=0.0441 | AUC=0.8388 | F1@0.5=0.5198 | F1@thr=0.5543 | thr=0.443 | pred_pos=0.154\n",
      "Fold 1 | Epoch 13 | loss=0.0426 | AUC=0.8409 | F1@0.5=0.5516 | F1@thr=0.5549 | thr=0.495 | pred_pos=0.147\n",
      "Fold 1 | Epoch 14 | loss=0.0408 | AUC=0.8362 | F1@0.5=0.5404 | F1@thr=0.5530 | thr=0.474 | pred_pos=0.151\n",
      "Fold 1 | Epoch 15 | loss=0.0395 | AUC=0.8345 | F1@0.5=0.5303 | F1@thr=0.5474 | thr=0.466 | pred_pos=0.151\n",
      "Fold 1 | Epoch 16 | loss=0.0384 | AUC=0.8307 | F1@0.5=0.5322 | F1@thr=0.5410 | thr=0.481 | pred_pos=0.146\n",
      "Fold 1 | Epoch 17 | loss=0.0369 | AUC=0.8298 | F1@0.5=0.5308 | F1@thr=0.5446 | thr=0.461 | pred_pos=0.148\n",
      "Fold 1 | Epoch 18 | loss=0.0357 | AUC=0.8338 | F1@0.5=0.5383 | F1@thr=0.5537 | thr=0.470 | pred_pos=0.159\n",
      "Fold 1 | Epoch 19 | loss=0.0346 | AUC=0.8360 | F1@0.5=0.5479 | F1@thr=0.5529 | thr=0.487 | pred_pos=0.151\n",
      "Fold 1 | Epoch 20 | loss=0.0331 | AUC=0.8351 | F1@0.5=0.5341 | F1@thr=0.5487 | thr=0.468 | pred_pos=0.146\n",
      "Saved: history_fold1.csv, Fold1_TrainingCurves.png\n",
      "\n",
      "Fold 2: model device: cuda:0\n",
      "Fold 2: n_train=157,649, n_test=33,630, pos_frac=0.1908, focal_alpha=0.8092\n",
      "Fold 2 | Epoch 01 | loss=0.0809 | AUC=0.8619 | F1@0.5=0.5640 | F1@thr=0.5851 | thr=0.447 | pred_pos=0.188\n",
      "Fold 2 | Epoch 02 | loss=0.0696 | AUC=0.8738 | F1@0.5=0.5889 | F1@thr=0.6074 | thr=0.436 | pred_pos=0.179\n",
      "Fold 2 | Epoch 03 | loss=0.0648 | AUC=0.8718 | F1@0.5=0.5998 | F1@thr=0.6044 | thr=0.470 | pred_pos=0.173\n",
      "Fold 2 | Epoch 04 | loss=0.0612 | AUC=0.8751 | F1@0.5=0.6096 | F1@thr=0.6144 | thr=0.473 | pred_pos=0.173\n",
      "Fold 2 | Epoch 05 | loss=0.0584 | AUC=0.8745 | F1@0.5=0.6015 | F1@thr=0.6145 | thr=0.453 | pred_pos=0.172\n",
      "Fold 2 | Epoch 06 | loss=0.0557 | AUC=0.8787 | F1@0.5=0.6157 | F1@thr=0.6164 | thr=0.476 | pred_pos=0.169\n",
      "Fold 2 | Epoch 07 | loss=0.0536 | AUC=0.8796 | F1@0.5=0.6032 | F1@thr=0.6185 | thr=0.437 | pred_pos=0.168\n",
      "Fold 2 | Epoch 08 | loss=0.0516 | AUC=0.8705 | F1@0.5=0.5935 | F1@thr=0.5993 | thr=0.457 | pred_pos=0.167\n",
      "Fold 2 | Epoch 09 | loss=0.0500 | AUC=0.8699 | F1@0.5=0.5983 | F1@thr=0.6092 | thr=0.460 | pred_pos=0.156\n",
      "Fold 2 | Epoch 10 | loss=0.0481 | AUC=0.8709 | F1@0.5=0.5820 | F1@thr=0.6032 | thr=0.433 | pred_pos=0.163\n",
      "Fold 2 | Epoch 11 | loss=0.0466 | AUC=0.8710 | F1@0.5=0.6027 | F1@thr=0.6021 | thr=0.502 | pred_pos=0.157\n",
      "Fold 2 | Epoch 12 | loss=0.0448 | AUC=0.8699 | F1@0.5=0.6097 | F1@thr=0.6095 | thr=0.494 | pred_pos=0.157\n",
      "Fold 2 | Epoch 13 | loss=0.0432 | AUC=0.8730 | F1@0.5=0.5981 | F1@thr=0.6059 | thr=0.469 | pred_pos=0.164\n",
      "Fold 2 | Epoch 14 | loss=0.0418 | AUC=0.8680 | F1@0.5=0.5847 | F1@thr=0.5988 | thr=0.445 | pred_pos=0.158\n",
      "Fold 2 | Epoch 15 | loss=0.0402 | AUC=0.8621 | F1@0.5=0.5927 | F1@thr=0.5922 | thr=0.490 | pred_pos=0.153\n",
      "Fold 2 | Epoch 16 | loss=0.0395 | AUC=0.8665 | F1@0.5=0.6004 | F1@thr=0.6002 | thr=0.516 | pred_pos=0.153\n",
      "Fold 2 | Epoch 17 | loss=0.0377 | AUC=0.8634 | F1@0.5=0.5901 | F1@thr=0.5884 | thr=0.493 | pred_pos=0.156\n",
      "Fold 2 | Epoch 18 | loss=0.0365 | AUC=0.8663 | F1@0.5=0.5902 | F1@thr=0.5924 | thr=0.491 | pred_pos=0.157\n",
      "Fold 2 | Epoch 19 | loss=0.0354 | AUC=0.8640 | F1@0.5=0.5911 | F1@thr=0.5913 | thr=0.497 | pred_pos=0.154\n",
      "Fold 2 | Epoch 20 | loss=0.0343 | AUC=0.8638 | F1@0.5=0.5845 | F1@thr=0.5867 | thr=0.480 | pred_pos=0.157\n",
      "Saved: history_fold2.csv, Fold2_TrainingCurves.png\n",
      "\n",
      "Fold 3: model device: cuda:0\n",
      "Fold 3: n_train=146,809, n_test=44,470, pos_frac=0.1784, focal_alpha=0.8216\n",
      "Fold 3 | Epoch 01 | loss=0.0820 | AUC=0.8557 | F1@0.5=0.5636 | F1@thr=0.6294 | thr=0.429 | pred_pos=0.209\n",
      "Fold 3 | Epoch 02 | loss=0.0705 | AUC=0.8690 | F1@0.5=0.5700 | F1@thr=0.6457 | thr=0.395 | pred_pos=0.221\n",
      "Fold 3 | Epoch 03 | loss=0.0655 | AUC=0.8691 | F1@0.5=0.5954 | F1@thr=0.6425 | thr=0.446 | pred_pos=0.188\n",
      "Fold 3 | Epoch 04 | loss=0.0612 | AUC=0.8723 | F1@0.5=0.6071 | F1@thr=0.6450 | thr=0.448 | pred_pos=0.196\n",
      "Fold 3 | Epoch 05 | loss=0.0580 | AUC=0.8704 | F1@0.5=0.6063 | F1@thr=0.6474 | thr=0.446 | pred_pos=0.193\n",
      "Fold 3 | Epoch 06 | loss=0.0551 | AUC=0.8792 | F1@0.5=0.6300 | F1@thr=0.6638 | thr=0.441 | pred_pos=0.197\n",
      "Fold 3 | Epoch 07 | loss=0.0528 | AUC=0.8732 | F1@0.5=0.6295 | F1@thr=0.6567 | thr=0.447 | pred_pos=0.200\n",
      "Fold 3 | Epoch 08 | loss=0.0503 | AUC=0.8687 | F1@0.5=0.6116 | F1@thr=0.6409 | thr=0.456 | pred_pos=0.182\n",
      "Fold 3 | Epoch 09 | loss=0.0480 | AUC=0.8696 | F1@0.5=0.6261 | F1@thr=0.6417 | thr=0.475 | pred_pos=0.184\n",
      "Fold 3 | Epoch 10 | loss=0.0463 | AUC=0.8676 | F1@0.5=0.6117 | F1@thr=0.6372 | thr=0.461 | pred_pos=0.182\n",
      "Fold 3 | Epoch 11 | loss=0.0446 | AUC=0.8699 | F1@0.5=0.6146 | F1@thr=0.6451 | thr=0.435 | pred_pos=0.191\n",
      "Fold 3 | Epoch 12 | loss=0.0423 | AUC=0.8657 | F1@0.5=0.6127 | F1@thr=0.6380 | thr=0.441 | pred_pos=0.194\n",
      "Fold 3 | Epoch 13 | loss=0.0409 | AUC=0.8610 | F1@0.5=0.6113 | F1@thr=0.6260 | thr=0.464 | pred_pos=0.180\n",
      "Fold 3 | Epoch 14 | loss=0.0393 | AUC=0.8637 | F1@0.5=0.6112 | F1@thr=0.6311 | thr=0.460 | pred_pos=0.181\n",
      "Fold 3 | Epoch 15 | loss=0.0376 | AUC=0.8610 | F1@0.5=0.6069 | F1@thr=0.6237 | thr=0.456 | pred_pos=0.187\n",
      "Fold 3 | Epoch 16 | loss=0.0363 | AUC=0.8626 | F1@0.5=0.6187 | F1@thr=0.6268 | thr=0.479 | pred_pos=0.173\n",
      "Fold 3 | Epoch 17 | loss=0.0349 | AUC=0.8656 | F1@0.5=0.6175 | F1@thr=0.6278 | thr=0.465 | pred_pos=0.182\n",
      "Fold 3 | Epoch 18 | loss=0.0335 | AUC=0.8626 | F1@0.5=0.6184 | F1@thr=0.6259 | thr=0.478 | pred_pos=0.177\n",
      "Fold 3 | Epoch 19 | loss=0.0323 | AUC=0.8597 | F1@0.5=0.6072 | F1@thr=0.6236 | thr=0.454 | pred_pos=0.176\n",
      "Fold 3 | Epoch 20 | loss=0.0314 | AUC=0.8564 | F1@0.5=0.5955 | F1@thr=0.6141 | thr=0.442 | pred_pos=0.176\n",
      "Saved: history_fold3.csv, Fold3_TrainingCurves.png\n",
      "\n",
      "Fold 4: model device: cuda:0\n",
      "Fold 4: n_train=147,945, n_test=43,334, pos_frac=0.1947, focal_alpha=0.8053\n",
      "Fold 4 | Epoch 01 | loss=0.0810 | AUC=0.8422 | F1@0.5=0.5219 | F1@thr=0.5536 | thr=0.434 | pred_pos=0.171\n",
      "Fold 4 | Epoch 02 | loss=0.0695 | AUC=0.8507 | F1@0.5=0.5361 | F1@thr=0.5747 | thr=0.439 | pred_pos=0.166\n",
      "Fold 4 | Epoch 03 | loss=0.0639 | AUC=0.8611 | F1@0.5=0.5594 | F1@thr=0.5849 | thr=0.441 | pred_pos=0.168\n",
      "Fold 4 | Epoch 04 | loss=0.0598 | AUC=0.8612 | F1@0.5=0.5569 | F1@thr=0.5884 | thr=0.432 | pred_pos=0.166\n",
      "Fold 4 | Epoch 05 | loss=0.0567 | AUC=0.8628 | F1@0.5=0.5580 | F1@thr=0.5886 | thr=0.412 | pred_pos=0.169\n",
      "Fold 4 | Epoch 06 | loss=0.0536 | AUC=0.8625 | F1@0.5=0.5648 | F1@thr=0.5908 | thr=0.444 | pred_pos=0.158\n",
      "Fold 4 | Epoch 07 | loss=0.0512 | AUC=0.8586 | F1@0.5=0.5501 | F1@thr=0.5832 | thr=0.424 | pred_pos=0.154\n",
      "Fold 4 | Epoch 08 | loss=0.0483 | AUC=0.8587 | F1@0.5=0.5734 | F1@thr=0.5891 | thr=0.455 | pred_pos=0.156\n",
      "Fold 4 | Epoch 09 | loss=0.0464 | AUC=0.8604 | F1@0.5=0.5878 | F1@thr=0.5879 | thr=0.484 | pred_pos=0.157\n",
      "Fold 4 | Epoch 10 | loss=0.0441 | AUC=0.8608 | F1@0.5=0.5805 | F1@thr=0.5866 | thr=0.446 | pred_pos=0.157\n",
      "Fold 4 | Epoch 11 | loss=0.0423 | AUC=0.8567 | F1@0.5=0.5774 | F1@thr=0.5835 | thr=0.469 | pred_pos=0.154\n",
      "Fold 4 | Epoch 12 | loss=0.0402 | AUC=0.8537 | F1@0.5=0.5678 | F1@thr=0.5725 | thr=0.459 | pred_pos=0.150\n",
      "Fold 4 | Epoch 13 | loss=0.0385 | AUC=0.8554 | F1@0.5=0.5788 | F1@thr=0.5777 | thr=0.507 | pred_pos=0.149\n",
      "Fold 4 | Epoch 14 | loss=0.0370 | AUC=0.8498 | F1@0.5=0.5591 | F1@thr=0.5678 | thr=0.471 | pred_pos=0.148\n",
      "Fold 4 | Epoch 15 | loss=0.0354 | AUC=0.8553 | F1@0.5=0.5774 | F1@thr=0.5803 | thr=0.465 | pred_pos=0.151\n",
      "Fold 4 | Epoch 16 | loss=0.0339 | AUC=0.8537 | F1@0.5=0.5760 | F1@thr=0.5764 | thr=0.513 | pred_pos=0.142\n",
      "Fold 4 | Epoch 17 | loss=0.0323 | AUC=0.8562 | F1@0.5=0.5734 | F1@thr=0.5835 | thr=0.464 | pred_pos=0.143\n",
      "Fold 4 | Epoch 18 | loss=0.0310 | AUC=0.8529 | F1@0.5=0.5747 | F1@thr=0.5776 | thr=0.464 | pred_pos=0.147\n",
      "Fold 4 | Epoch 19 | loss=0.0302 | AUC=0.8532 | F1@0.5=0.5803 | F1@thr=0.5812 | thr=0.494 | pred_pos=0.144\n",
      "Fold 4 | Epoch 20 | loss=0.0290 | AUC=0.8527 | F1@0.5=0.5661 | F1@thr=0.5733 | thr=0.474 | pred_pos=0.142\n",
      "Saved: history_fold4.csv, Fold4_TrainingCurves.png\n",
      "\n",
      "Fold 5: model device: cuda:0\n",
      "Fold 5: n_train=152,008, n_test=39,271, pos_frac=0.1795, focal_alpha=0.8205\n",
      "Fold 5 | Epoch 01 | loss=0.0817 | AUC=0.8653 | F1@0.5=0.5799 | F1@thr=0.6383 | thr=0.416 | pred_pos=0.188\n",
      "Fold 5 | Epoch 02 | loss=0.0710 | AUC=0.8814 | F1@0.5=0.6363 | F1@thr=0.6617 | thr=0.440 | pred_pos=0.209\n",
      "Fold 5 | Epoch 03 | loss=0.0664 | AUC=0.8767 | F1@0.5=0.5882 | F1@thr=0.6510 | thr=0.417 | pred_pos=0.188\n",
      "Fold 5 | Epoch 04 | loss=0.0625 | AUC=0.8831 | F1@0.5=0.6199 | F1@thr=0.6590 | thr=0.435 | pred_pos=0.200\n",
      "Fold 5 | Epoch 05 | loss=0.0596 | AUC=0.8852 | F1@0.5=0.6456 | F1@thr=0.6663 | thr=0.449 | pred_pos=0.199\n",
      "Fold 5 | Epoch 06 | loss=0.0573 | AUC=0.8820 | F1@0.5=0.6596 | F1@thr=0.6643 | thr=0.468 | pred_pos=0.200\n",
      "Fold 5 | Epoch 07 | loss=0.0551 | AUC=0.8791 | F1@0.5=0.6258 | F1@thr=0.6634 | thr=0.414 | pred_pos=0.201\n",
      "Fold 5 | Epoch 08 | loss=0.0530 | AUC=0.8792 | F1@0.5=0.6262 | F1@thr=0.6645 | thr=0.418 | pred_pos=0.191\n",
      "Fold 5 | Epoch 09 | loss=0.0512 | AUC=0.8814 | F1@0.5=0.6562 | F1@thr=0.6673 | thr=0.457 | pred_pos=0.194\n",
      "Fold 5 | Epoch 10 | loss=0.0494 | AUC=0.8784 | F1@0.5=0.6376 | F1@thr=0.6591 | thr=0.451 | pred_pos=0.195\n",
      "Fold 5 | Epoch 11 | loss=0.0476 | AUC=0.8811 | F1@0.5=0.6391 | F1@thr=0.6628 | thr=0.427 | pred_pos=0.198\n",
      "Fold 5 | Epoch 12 | loss=0.0458 | AUC=0.8802 | F1@0.5=0.6277 | F1@thr=0.6610 | thr=0.435 | pred_pos=0.190\n",
      "Fold 5 | Epoch 13 | loss=0.0441 | AUC=0.8751 | F1@0.5=0.6405 | F1@thr=0.6531 | thr=0.475 | pred_pos=0.184\n",
      "Fold 5 | Epoch 14 | loss=0.0425 | AUC=0.8763 | F1@0.5=0.6571 | F1@thr=0.6569 | thr=0.511 | pred_pos=0.188\n",
      "Fold 5 | Epoch 15 | loss=0.0412 | AUC=0.8696 | F1@0.5=0.6269 | F1@thr=0.6391 | thr=0.470 | pred_pos=0.190\n",
      "Fold 5 | Epoch 16 | loss=0.0393 | AUC=0.8729 | F1@0.5=0.6497 | F1@thr=0.6456 | thr=0.522 | pred_pos=0.188\n",
      "Fold 5 | Epoch 17 | loss=0.0379 | AUC=0.8797 | F1@0.5=0.6553 | F1@thr=0.6565 | thr=0.493 | pred_pos=0.188\n",
      "Fold 5 | Epoch 18 | loss=0.0367 | AUC=0.8717 | F1@0.5=0.6463 | F1@thr=0.6519 | thr=0.466 | pred_pos=0.190\n",
      "Fold 5 | Epoch 19 | loss=0.0350 | AUC=0.8739 | F1@0.5=0.6545 | F1@thr=0.6554 | thr=0.495 | pred_pos=0.192\n",
      "Fold 5 | Epoch 20 | loss=0.0337 | AUC=0.8710 | F1@0.5=0.6418 | F1@thr=0.6427 | thr=0.489 | pred_pos=0.185\n",
      "Saved: history_fold5.csv, Fold5_TrainingCurves.png\n",
      "\n",
      "=== Gene-disjoint CV summary (mean ± std) ===\n",
      "                       mean          std\n",
      "n              38255.800000  6038.275929\n",
      "roc_auc            0.855801     0.013556\n",
      "pr_auc             0.651991     0.048053\n",
      "acc                0.860013     0.009665\n",
      "precision          0.636350     0.042450\n",
      "recall             0.556267     0.039660\n",
      "f1                 0.593089     0.036416\n",
      "thr                0.470717     0.017968\n",
      "f1_0p5             0.584398     0.039635\n",
      "recall_0p5         0.522224     0.052247\n",
      "precision_0p5      0.667618     0.049605\n",
      "\n",
      "=== Variant-type stratified (mean over folds) ===\n",
      "                       roc_auc    pr_auc       acc  precision    recall  \\\n",
      "group                                                                     \n",
      "synonymous            0.665015  0.003613  0.947783   0.003345  0.153107   \n",
      "intronic              0.742698  0.057794  0.910101   0.067993  0.339851   \n",
      "missense              0.650992  0.431730  0.718588   0.482630  0.272353   \n",
      "frameshift            0.544507  0.987169  0.841922   0.987076  0.851031   \n",
      "other                 0.773997  0.798940  0.650036   0.835300  0.484446   \n",
      "nonsense/stop_gained  0.599997  0.988035  0.432989   0.988362  0.428999   \n",
      "\n",
      "                            f1        n  \n",
      "group                                    \n",
      "synonymous            0.006530  15945.2  \n",
      "intronic              0.112537  11066.8  \n",
      "missense              0.346925   4696.8  \n",
      "frameshift            0.913746   2636.6  \n",
      "other                 0.612727   2148.4  \n",
      "nonsense/stop_gained  0.598062   1762.0  \n",
      "\n",
      "Saved: gene_disjoint_fold_metrics.csv\n",
      "Saved: variant_type_fold_metrics.csv\n",
      "Saved: CV_ROC_mean.png, CV_PR_mean.png\n",
      "Saved: CV_ConfusionMatrix_sum.png, CV_ConfusionMatrix_sum_norm.png\n",
      "Saved: VariantType_F1_bar.png, VariantType_PRAUC_bar.png\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# ِAuthor: Ahmed\n",
    "# ClinVar v2: Gene-disjoint (StratifiedGroupKFold) CV\n",
    "# REF/ALT-aware windows + Dual-branch (CGR-2D + OneHot-1D)\n",
    "# Optimized for: RTX 3080 (CUDA) + Threadripper 3990X\n",
    "#\n",
    "# ✅ Works in Jupyter on Windows (forces num_workers=0 to avoid spawn/pickle crash)\n",
    "# ✅ Works as .py script (uses multi-worker DataLoader for CPU preproc speed)\n",
    "#\n",
    "# Edit REF_FA and VCF_PATH .\n",
    "# ==============================================================\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from pyfaidx import Fasta\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    precision_recall_curve, roc_curve, confusion_matrix\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------\n",
    "# 0) CONFIG (EDIT THESE PATHS)\n",
    "# ------------------------------\n",
    "REF_FA   = r\"C:\\Users\\muham\\_Projects\\DNA\\ref\\GRCh38.fa\"   # <-- set yours\n",
    "VCF_PATH = r\"clinvar.vcf\"                                  # <-- set yours\n",
    "\n",
    "# Data / featurization\n",
    "WINDOW_WIDTH = 201\n",
    "CGR_K = 6                     # 2^6=64 grid (fast). Try 7 later if you want.\n",
    "MAX_VCF_ROWS = 500_000         # how many VCF rows to scan\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "N_SPLITS = 5\n",
    "\n",
    "# IO / cache\n",
    "CACHE_PARQUET = \"df_model_500k.parquet\"\n",
    "SAVE_FOLD_CSV = \"gene_disjoint_fold_metrics.csv\"\n",
    "SAVE_TYPE_CSV = \"variant_type_fold_metrics.csv\"\n",
    "\n",
    "# Plot settings\n",
    "FIG_DPI = 300   # set 600 if you want higher resolution figures\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Utils: environment + device\n",
    "# ------------------------------\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def in_notebook() -> bool:\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        ip = get_ipython()\n",
    "        return (ip is not None) and (\"IPKernelApp\" in ip.config)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "class DummyScaler:\n",
    "    \"\"\"Fallback when AMP is disabled (CPU or user choice).\"\"\"\n",
    "    def scale(self, loss): return loss\n",
    "    def step(self, optimizer): optimizer.step()\n",
    "    def update(self): pass\n",
    "\n",
    "def setup_device():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    use_amp = (device.type == \"cuda\")\n",
    "    if use_amp:\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        try:\n",
    "            torch.set_float32_matmul_precision(\"high\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        scaler = torch.amp.GradScaler(\"cuda\", enabled=True)\n",
    "    else:\n",
    "        scaler = DummyScaler()\n",
    "\n",
    "    return device, use_amp, scaler\n",
    "\n",
    "def best_num_workers():\n",
    "    # In Jupyter on Windows, multi-worker DataLoader often crashes due to pickling in __main__\n",
    "    if os.name == \"nt\" and in_notebook():\n",
    "        return 0\n",
    "    # Script mode: Threadripper can help; keep it moderate to avoid RAM thrash.\n",
    "    cpu = os.cpu_count() or 8\n",
    "    return max(4, min(16, cpu // 4))\n",
    "\n",
    "# ------------------------------\n",
    "# 2) ClinVar parsing helpers\n",
    "# ------------------------------\n",
    "def parse_info(info_str: str) -> dict:\n",
    "    out = {\"CLNSIG\":\"\", \"CLNVC\":\"\", \"MC\":\"\", \"GENEINFO\":\"\", \"SYMBOL\":\"\"}\n",
    "    if not isinstance(info_str, str):\n",
    "        return out\n",
    "    fields = {}\n",
    "    for item in info_str.split(\";\"):\n",
    "        if \"=\" in item:\n",
    "            k, v = item.split(\"=\", 1)\n",
    "            fields[k] = v\n",
    "    for k in [\"CLNSIG\", \"CLNVC\", \"MC\", \"GENEINFO\"]:\n",
    "        if k in fields:\n",
    "            out[k] = fields[k]\n",
    "    gi = out[\"GENEINFO\"]\n",
    "    if gi:\n",
    "        # format often: GENE:ID|GENE2:ID2...\n",
    "        out[\"SYMBOL\"] = gi.split(\"|\")[0].split(\":\")[0]\n",
    "    return out\n",
    "\n",
    "def normalize_chrom(chrom: str) -> str:\n",
    "    chrom = str(chrom)\n",
    "    if chrom.startswith(\"chr\"):\n",
    "        chrom = chrom[3:]\n",
    "    if chrom == \"M\":\n",
    "        chrom = \"MT\"\n",
    "    return chrom\n",
    "\n",
    "def label_from_clnsig(clnsig: str):\n",
    "    \"\"\"\n",
    "    Label policy:\n",
    "      - Positive: (Likely) Pathogenic\n",
    "      - Negative: (Likely) Benign\n",
    "      - Exclude: VUS / conflicting / uncertain or mixed benign+pathogenic\n",
    "    \"\"\"\n",
    "    s = str(clnsig).lower()\n",
    "    s = s.replace(\" \", \"_\").replace(\"/\", \"_\").replace(\",\", \"_\")\n",
    "    if \"conflicting\" in s or \"uncertain\" in s:\n",
    "        return None\n",
    "\n",
    "    has_path = (\"pathogenic\" in s)\n",
    "    has_ben  = (\"benign\" in s)\n",
    "\n",
    "    # Mixed signals -> exclude\n",
    "    if has_path and has_ben:\n",
    "        return None\n",
    "\n",
    "    if has_path:\n",
    "        return 1\n",
    "    if has_ben:\n",
    "        return 0\n",
    "    return None\n",
    "\n",
    "def tag_variant_group(mc: str) -> str:\n",
    "    mc = str(mc).lower()\n",
    "    if \"missense_variant\" in mc:\n",
    "        return \"missense\"\n",
    "    if \"frameshift_variant\" in mc:\n",
    "        return \"frameshift\"\n",
    "    if \"stop_gained\" in mc or \"nonsense\" in mc:\n",
    "        return \"nonsense/stop_gained\"\n",
    "    if \"synonymous_variant\" in mc:\n",
    "        return \"synonymous\"\n",
    "    if \"intron_variant\" in mc:\n",
    "        return \"intronic\"\n",
    "    return \"other\"\n",
    "\n",
    "# ------------------------------\n",
    "# 3) REF/ALT-aware window extraction\n",
    "# ------------------------------\n",
    "def get_ref_window(ref_fa, chrom: str, pos_1based: int, width: int):\n",
    "    chrom = normalize_chrom(chrom)\n",
    "    if chrom not in ref_fa:\n",
    "        return None\n",
    "    half = width // 2\n",
    "    center0 = pos_1based - 1\n",
    "    start0 = center0 - half\n",
    "    end0   = center0 + half + 1\n",
    "    if start0 < 0 or end0 > len(ref_fa[chrom]):\n",
    "        return None\n",
    "    seq = ref_fa[chrom][start0:end0]\n",
    "    seq = str(seq).upper()\n",
    "    if len(seq) != width or \"N\" in seq:\n",
    "        return None\n",
    "    return seq, half, start0, end0\n",
    "\n",
    "def make_ref_alt_window(ref_fa, chrom: str, pos_1based: int, ref_allele: str, alt_allele: str, width: int):\n",
    "    w = get_ref_window(ref_fa, chrom, pos_1based, width)\n",
    "    if w is None:\n",
    "        return None\n",
    "    seq_ref, center, start0, end0 = w\n",
    "\n",
    "    ref_allele = str(ref_allele).upper()\n",
    "    alt_allele = str(alt_allele).upper()\n",
    "\n",
    "    # multi-ALT -> take first\n",
    "    if \",\" in alt_allele:\n",
    "        alt_allele = alt_allele.split(\",\")[0]\n",
    "\n",
    "    # symbolic ALT, missing, etc.\n",
    "    if alt_allele in {\".\", \"*\", \"\"}:\n",
    "        return None\n",
    "\n",
    "    # Verify REF allele matches reference at center\n",
    "    if seq_ref[center:center+len(ref_allele)] != ref_allele:\n",
    "        return None\n",
    "\n",
    "    # Apply edit (supports SNP / indel)\n",
    "    seq_alt_raw = seq_ref[:center] + alt_allele + seq_ref[center+len(ref_allele):]\n",
    "\n",
    "    # Force fixed width: truncate or pad by reading extra reference after end0\n",
    "    if len(seq_alt_raw) > width:\n",
    "        seq_alt = seq_alt_raw[:width]\n",
    "    elif len(seq_alt_raw) < width:\n",
    "        need = width - len(seq_alt_raw)\n",
    "        chrom_n = normalize_chrom(chrom)\n",
    "        extra = str(ref_fa[chrom_n][end0:end0+need]).upper()\n",
    "        if len(extra) != need or \"N\" in extra:\n",
    "            return None\n",
    "        seq_alt = seq_alt_raw + extra\n",
    "    else:\n",
    "        seq_alt = seq_alt_raw\n",
    "\n",
    "    if len(seq_alt) != width or \"N\" in seq_alt:\n",
    "        return None\n",
    "\n",
    "    return seq_ref, seq_alt\n",
    "\n",
    "# ------------------------------\n",
    "# 4) Build dataset dataframe (chunked)\n",
    "# ------------------------------\n",
    "def build_variant_df(vcf_path: str, ref_fa, max_rows: int, width: int) -> pd.DataFrame:\n",
    "    usecols = [0, 1, 3, 4, 7]\n",
    "    names = [\"CHROM\", \"POS\", \"REF\", \"ALT\", \"INFO\"]\n",
    "\n",
    "    records = []\n",
    "    rows_seen = 0\n",
    "    chunksize = 100_000\n",
    "\n",
    "    t0 = time.time()\n",
    "    reader = pd.read_csv(\n",
    "        vcf_path,\n",
    "        sep=\"\\t\",\n",
    "        comment=\"#\",\n",
    "        header=None,\n",
    "        usecols=usecols,\n",
    "        names=names,\n",
    "        dtype=str,\n",
    "        engine=\"c\",\n",
    "        chunksize=chunksize\n",
    "    )\n",
    "\n",
    "    for chunk in reader:\n",
    "        if rows_seen >= max_rows:\n",
    "            break\n",
    "\n",
    "        remain = max_rows - rows_seen\n",
    "        if len(chunk) > remain:\n",
    "            chunk = chunk.iloc[:remain]\n",
    "\n",
    "        rows_seen += len(chunk)\n",
    "\n",
    "        for row in chunk.itertuples(index=False):\n",
    "            chrom, pos_s, ref_a, alt_a, info_s = row\n",
    "\n",
    "            try:\n",
    "                pos = int(pos_s)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            info_f = parse_info(info_s)\n",
    "            y = label_from_clnsig(info_f[\"CLNSIG\"])\n",
    "            if y is None:\n",
    "                continue\n",
    "\n",
    "            pair = make_ref_alt_window(ref_fa, chrom, pos, ref_a, alt_a, width=width)\n",
    "            if pair is None:\n",
    "                continue\n",
    "            seq_ref, seq_alt = pair\n",
    "\n",
    "            records.append({\n",
    "                \"CHROM\": normalize_chrom(chrom),\n",
    "                \"POS\": pos,\n",
    "                \"REF\": ref_a,\n",
    "                \"ALT\": alt_a,\n",
    "                \"CLNSIG\": info_f[\"CLNSIG\"],\n",
    "                \"CLASS\": int(y),\n",
    "                \"SYMBOL\": info_f[\"SYMBOL\"] if info_f[\"SYMBOL\"] else \"UNK\",\n",
    "                \"CLNVC\": info_f[\"CLNVC\"],\n",
    "                \"MC\": info_f[\"MC\"],\n",
    "                \"seq_ref\": seq_ref,\n",
    "                \"seq_alt\": seq_alt\n",
    "            })\n",
    "\n",
    "        print(f\"Scanned {rows_seen:,} VCF rows | kept {len(records):,} labeled so far...\")\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    dt = time.time() - t0\n",
    "    print(f\"\\nFinished scan: {rows_seen:,} VCF rows in {dt:.1f}s\")\n",
    "    print(f\"Kept {len(df):,} labeled allele-aware variants\")\n",
    "    print(\"Pathogenic fraction:\", df[\"CLASS\"].mean() if len(df) else np.nan)\n",
    "    return df\n",
    "\n",
    "def filter_acgt(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    mask_ref = df[\"seq_ref\"].str.fullmatch(r\"[ACGT]+\")\n",
    "    mask_alt = df[\"seq_alt\"].str.fullmatch(r\"[ACGT]+\")\n",
    "    bad = (~mask_ref) | (~mask_alt)\n",
    "    bad_n = int(bad.sum())\n",
    "    print(\"Bad rows (non-ACGT):\", bad_n)\n",
    "    if bad_n > 0:\n",
    "        ex = df.loc[bad, \"seq_alt\"].head(5).tolist()\n",
    "        print(\"Example bad seq_alt:\", ex)\n",
    "    df2 = df.loc[~bad].reset_index(drop=True)\n",
    "    print(\"After filtering:\", len(df2))\n",
    "    return df2\n",
    "\n",
    "# ------------------------------\n",
    "# 5) Representations: CGR + OneHot (ref+alt stacked)\n",
    "# ------------------------------\n",
    "def sequence_to_cgr(seq: str, k: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Fast CGR via direct k-mer bit mapping.\n",
    "    A:(0,0), C:(0,1), G:(1,1), T:(1,0)\n",
    "    Output: (2^k, 2^k) float32 normalized.\n",
    "    \"\"\"\n",
    "    mapping = {'A': (0,0), 'C': (0,1), 'G': (1,1), 'T': (1,0)}\n",
    "    N = 2 ** k\n",
    "\n",
    "    seq = str(seq).upper()\n",
    "    if len(seq) < k:\n",
    "        seq = seq.ljust(k, 'A')\n",
    "\n",
    "    xb = np.fromiter((mapping.get(b, (0,0))[0] for b in seq), dtype=np.uint8, count=len(seq))\n",
    "    yb = np.fromiter((mapping.get(b, (0,0))[1] for b in seq), dtype=np.uint8, count=len(seq))\n",
    "    weights = (1 << np.arange(k-1, -1, -1, dtype=np.int32))\n",
    "\n",
    "    try:\n",
    "        from numpy.lib.stride_tricks import sliding_window_view\n",
    "        xw = sliding_window_view(xb, k)\n",
    "        yw = sliding_window_view(yb, k)\n",
    "        x_idx = (xw * weights).sum(axis=1).astype(np.int32)\n",
    "        y_idx = (yw * weights).sum(axis=1).astype(np.int32)\n",
    "        freq = np.zeros((N, N), dtype=np.float32)\n",
    "        np.add.at(freq, (y_idx, x_idx), 1.0)\n",
    "    except Exception:\n",
    "        freq = np.zeros((N, N), dtype=np.float32)\n",
    "        for i in range(len(seq) - k + 1):\n",
    "            xi = int((xb[i:i+k] * weights).sum())\n",
    "            yi = int((yb[i:i+k] * weights).sum())\n",
    "            freq[yi, xi] += 1.0\n",
    "\n",
    "    m = float(freq.max())\n",
    "    if m > 0:\n",
    "        freq /= m\n",
    "    return torch.tensor(freq, dtype=torch.float32)\n",
    "\n",
    "def one_hot(seq: str, width: int) -> torch.Tensor:\n",
    "    base_to_idx = {'A':0, 'C':1, 'G':2, 'T':3}\n",
    "    seq = str(seq).upper()\n",
    "    mat = torch.zeros((4, width), dtype=torch.float32)\n",
    "    for j, b in enumerate(seq[:width]):\n",
    "        i = base_to_idx.get(b, None)\n",
    "        if i is not None:\n",
    "            mat[i, j] = 1.0\n",
    "    return mat\n",
    "\n",
    "class VariantDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, k: int, width: int):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.k = k\n",
    "        self.width = width\n",
    "        self.N = 2 ** k\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r = self.df.iloc[idx]\n",
    "        sref = r[\"seq_ref\"]\n",
    "        salt = r[\"seq_alt\"]\n",
    "\n",
    "        cgr_ref = sequence_to_cgr(sref, self.k).unsqueeze(0)  # (1,N,N)\n",
    "        cgr_alt = sequence_to_cgr(salt, self.k).unsqueeze(0)  # (1,N,N)\n",
    "        img = torch.cat([cgr_ref, cgr_alt], dim=0)            # (2,N,N)\n",
    "\n",
    "        oh_ref = one_hot(sref, self.width)                    # (4,width)\n",
    "        oh_alt = one_hot(salt, self.width)                    # (4,width)\n",
    "        seq = torch.cat([oh_ref, oh_alt], dim=0)              # (8,width)\n",
    "\n",
    "        y = torch.tensor(float(r[\"CLASS\"]), dtype=torch.float32)\n",
    "        return img, seq, y\n",
    "\n",
    "# ------------------------------\n",
    "# 6) Model: dual branch\n",
    "# ------------------------------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = float(alpha)\n",
    "        self.gamma = float(gamma)\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, labels, reduction=\"none\")\n",
    "        pt = torch.exp(-bce)\n",
    "        loss = self.alpha * (1.0 - pt) ** self.gamma * bce\n",
    "        return loss.mean()\n",
    "\n",
    "class DualBranchModel(nn.Module):\n",
    "    def __init__(self, k: int, seq_len: int):\n",
    "        super().__init__()\n",
    "        N = 2 ** k\n",
    "\n",
    "        self.cnn2d = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.cnn1d = nn.Sequential(\n",
    "            nn.Conv1d(8, 16, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, 2),\n",
    "            nn.Conv1d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2, 2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            d1 = self.cnn2d(torch.zeros(1, 2, N, N)).shape[1]\n",
    "            d2 = self.cnn1d(torch.zeros(1, 8, seq_len)).shape[1]\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d1 + d2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, seq):\n",
    "        f_img = self.cnn2d(img)\n",
    "        f_seq = self.cnn1d(seq)\n",
    "        f = torch.cat([f_img, f_seq], dim=1)\n",
    "        return self.mlp(f).view(-1)\n",
    "\n",
    "# ------------------------------\n",
    "# 7) Metrics + threshold\n",
    "# ------------------------------\n",
    "def predict_probs(model, loader, device):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for img, seq, y in loader:\n",
    "            img = img.to(device, non_blocking=True)\n",
    "            seq = seq.to(device, non_blocking=True)\n",
    "            logits = model(img, seq)\n",
    "            prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            ys.append(y.detach().cpu().numpy())\n",
    "            ps.append(prob)\n",
    "    return np.concatenate(ys), np.concatenate(ps)\n",
    "\n",
    "def metrics_binary(y_true, y_prob, thr=0.5):\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "    return {\n",
    "        \"n\": len(y_true),\n",
    "        \"roc_auc\": roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else np.nan,\n",
    "        \"pr_auc\": average_precision_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else np.nan,\n",
    "        \"acc\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "\n",
    "def best_threshold_f1(y_true: np.ndarray, y_prob: np.ndarray) -> float:\n",
    "    p, r, th = precision_recall_curve(y_true, y_prob)\n",
    "    f1 = (2 * p * r) / (p + r + 1e-9)\n",
    "    if th is None or len(th) == 0:\n",
    "        return 0.5\n",
    "    best_i = int(np.argmax(f1[1:]))  # skip first point\n",
    "    return float(th[best_i])\n",
    "\n",
    "# ------------------------------\n",
    "# 8) Plotting helpers\n",
    "# ------------------------------\n",
    "def save_fig(path, dpi=FIG_DPI):\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_fold_training_curves(history_df, out_png, fold):\n",
    "    epochs = history_df[\"epoch\"].values\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Loss (train)\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(epochs, history_df[\"loss\"].values, label=\"Train Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Fold {fold} - Train Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # AUC (on fold test)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(epochs, history_df[\"auc\"].values, label=\"Test ROC-AUC\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"AUC\")\n",
    "    plt.title(f\"Fold {fold} - ROC-AUC (Test)\")\n",
    "    plt.legend()\n",
    "\n",
    "    # F1 curves (on fold test)\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(epochs, history_df[\"f1_0p5\"].values, label=\"F1 @ 0.5 (Test)\")\n",
    "    plt.plot(epochs, history_df[\"f1_thr\"].values, label=\"F1 @ train-chosen thr (Test)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"F1\")\n",
    "    plt.title(f\"Fold {fold} - F1 (Test)\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Threshold + predicted positives\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(epochs, history_df[\"thr\"].values, label=\"thr (from train)\")\n",
    "    plt.plot(epochs, history_df[\"pred_pos\"].values, label=\"pred_pos (test)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.title(f\"Fold {fold} - Threshold / Pred Pos\")\n",
    "    plt.legend()\n",
    "\n",
    "    save_fig(out_png, dpi=FIG_DPI)\n",
    "\n",
    "def plot_mean_roc(curves, out_png):\n",
    "    mean_fpr = np.linspace(0, 1, 500)\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "\n",
    "    for c in curves:\n",
    "        fpr, tpr = c[\"fpr\"], c[\"tpr\"]\n",
    "        tpr_i = np.interp(mean_fpr, fpr, tpr)\n",
    "        tpr_i[0] = 0.0\n",
    "        tprs.append(tpr_i)\n",
    "        aucs.append(c[\"auc\"])\n",
    "\n",
    "    tprs = np.vstack(tprs)\n",
    "    mean_tpr = tprs.mean(axis=0)\n",
    "    std_tpr = tprs.std(axis=0)\n",
    "\n",
    "    mean_auc = np.mean(aucs)\n",
    "    std_auc = np.std(aucs)\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.plot(mean_fpr, mean_tpr, label=f\"Mean ROC (AUC={mean_auc:.3f}±{std_auc:.3f})\")\n",
    "    plt.fill_between(mean_fpr, np.maximum(mean_tpr - std_tpr, 0), np.minimum(mean_tpr + std_tpr, 1), alpha=0.2)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Gene-disjoint CV - ROC (mean ± std)\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    save_fig(out_png, dpi=FIG_DPI)\n",
    "\n",
    "def plot_mean_pr(curves, prevalence, out_png):\n",
    "    mean_recall = np.linspace(0, 1, 500)\n",
    "    precs = []\n",
    "    aps = []\n",
    "\n",
    "    for c in curves:\n",
    "        prec, rec = c[\"precision\"], c[\"recall\"]\n",
    "        # ensure monotonic recall for interpolation\n",
    "        order = np.argsort(rec)\n",
    "        rec_s = rec[order]\n",
    "        prec_s = prec[order]\n",
    "        p_i = np.interp(mean_recall, rec_s, prec_s)\n",
    "        precs.append(p_i)\n",
    "        aps.append(c[\"ap\"])\n",
    "\n",
    "    precs = np.vstack(precs)\n",
    "    mean_p = precs.mean(axis=0)\n",
    "    std_p = precs.std(axis=0)\n",
    "\n",
    "    mean_ap = np.mean(aps)\n",
    "    std_ap = np.std(aps)\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.plot(mean_recall, mean_p, label=f\"Mean PR (AP={mean_ap:.3f}±{std_ap:.3f})\")\n",
    "    plt.fill_between(mean_recall, np.maximum(mean_p - std_p, 0), np.minimum(mean_p + std_p, 1), alpha=0.2)\n",
    "    plt.hlines(prevalence, 0, 1, linestyles=\"--\", linewidth=1, label=f\"Baseline (prev={prevalence:.3f})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Gene-disjoint CV - Precision–Recall (mean ± std)\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    save_fig(out_png, dpi=FIG_DPI)\n",
    "\n",
    "def plot_confmat(cm, out_png, normalize=False, title=\"Confusion Matrix\"):\n",
    "    cm_plot = cm.astype(np.float64) if normalize else cm.copy()\n",
    "    if normalize:\n",
    "        cm_plot = cm_plot / (cm_plot.sum(axis=1, keepdims=True) + 1e-9)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm_plot, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, [\"Benign\", \"Pathogenic\"])\n",
    "    plt.yticks(tick_marks, [\"Benign\", \"Pathogenic\"])\n",
    "\n",
    "    fmt = \".3f\" if normalize else \"d\"\n",
    "    thresh = cm_plot.max() / 2.0 if cm_plot.size else 0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            val = cm_plot[i, j]\n",
    "            plt.text(j, i, format(val, fmt),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if val > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    save_fig(out_png, dpi=FIG_DPI)\n",
    "\n",
    "def plot_variant_type_bars(type_df, metric, out_png, title):\n",
    "    g = type_df.groupby(\"group\")\n",
    "    means = g[metric].mean()\n",
    "    stds = g[metric].std().fillna(0.0)\n",
    "    ns = g[\"n\"].mean()\n",
    "\n",
    "    order = ns.sort_values(ascending=False).index.tolist()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    x = np.arange(len(order))\n",
    "    plt.bar(x, means[order].values, yerr=stds[order].values, capsize=3)\n",
    "    plt.xticks(x, order, rotation=25, ha=\"right\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(title)\n",
    "    save_fig(out_png, dpi=FIG_DPI)\n",
    "\n",
    "# ------------------------------\n",
    "# 9) Main training + CV + plots\n",
    "# ------------------------------\n",
    "def main():\n",
    "    set_seed(SEED)\n",
    "    device, use_amp, scaler = setup_device()\n",
    "\n",
    "    # DataLoader tuning\n",
    "    NUM_WORKERS = best_num_workers()\n",
    "    PIN_MEMORY = (device.type == \"cuda\")\n",
    "    PERSISTENT = (NUM_WORKERS > 0)\n",
    "    PREFETCH = 4 if (NUM_WORKERS > 0) else None\n",
    "    print(f\"DataLoader: num_workers={NUM_WORKERS}, pin_memory={PIN_MEMORY}, persistent={PERSISTENT}\")\n",
    "\n",
    "    # Load reference FASTA\n",
    "    print(\"Loading reference FASTA...\")\n",
    "    ref_fa = Fasta(REF_FA, as_raw=True, sequence_always_upper=True)\n",
    "\n",
    "    # Build/load df_model\n",
    "    if os.path.exists(CACHE_PARQUET):\n",
    "        print(f\"Loading cache: {CACHE_PARQUET}\")\n",
    "        df_model = pd.read_parquet(CACHE_PARQUET)\n",
    "        print(\"Loaded cached variants:\", len(df_model))\n",
    "    else:\n",
    "        print(f\"Building dataset from {VCF_PATH} (up to {MAX_VCF_ROWS:,} rows)...\")\n",
    "        df_model = build_variant_df(VCF_PATH, ref_fa, max_rows=MAX_VCF_ROWS, width=WINDOW_WIDTH)\n",
    "        print(f\"Saving cache: {CACHE_PARQUET}\")\n",
    "        df_model.to_parquet(CACHE_PARQUET, index=False)\n",
    "\n",
    "    # Filter non-ACGT\n",
    "    df_model = filter_acgt(df_model)\n",
    "\n",
    "    same = float((df_model[\"seq_ref\"] == df_model[\"seq_alt\"]).mean()) if len(df_model) else 0.0\n",
    "    print(f\"Sanity: fraction where seq_ref == seq_alt: {same:.6f} (should be near 0)\")\n",
    "\n",
    "    prevalence_all = float(df_model[\"CLASS\"].mean()) if len(df_model) else 0.0\n",
    "\n",
    "    # Prepare CV\n",
    "    X_idx = np.arange(len(df_model))\n",
    "    y = df_model[\"CLASS\"].values.astype(int)\n",
    "    groups = df_model[\"SYMBOL\"].fillna(\"UNK\").values\n",
    "\n",
    "    cv = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    fold_rows = []\n",
    "    per_type_rows = []\n",
    "\n",
    "    roc_curves = []\n",
    "    pr_curves = []\n",
    "    cm_sum = np.zeros((2, 2), dtype=np.int64)\n",
    "\n",
    "    def make_loader(df, shuffle):\n",
    "        ds = VariantDataset(df, k=CGR_K, width=WINDOW_WIDTH)\n",
    "        kwargs = dict(\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=NUM_WORKERS,\n",
    "            pin_memory=PIN_MEMORY,\n",
    "            persistent_workers=PERSISTENT,\n",
    "        )\n",
    "        if NUM_WORKERS > 0 and PREFETCH is not None:\n",
    "            kwargs[\"prefetch_factor\"] = PREFETCH\n",
    "        return DataLoader(ds, **kwargs)\n",
    "\n",
    "    for fold, (tr, te) in enumerate(cv.split(X_idx, y, groups), 1):\n",
    "        df_tr = df_model.iloc[tr].reset_index(drop=True)\n",
    "        df_te = df_model.iloc[te].reset_index(drop=True)\n",
    "\n",
    "        # gene-disjoint check\n",
    "        assert set(df_tr[\"SYMBOL\"]).isdisjoint(set(df_te[\"SYMBOL\"]))\n",
    "\n",
    "        train_loader = make_loader(df_tr, shuffle=True)\n",
    "        test_loader = make_loader(df_te, shuffle=False)\n",
    "        train_eval_loader = make_loader(df_tr, shuffle=False)\n",
    "\n",
    "        model = DualBranchModel(k=CGR_K, seq_len=WINDOW_WIDTH).to(device)\n",
    "        print(f\"\\nFold {fold}: model device: {next(model.parameters()).device}\")\n",
    "\n",
    "        pos_frac = float(df_tr[\"CLASS\"].mean())\n",
    "        alpha = float(1.0 - pos_frac)  # upweight positives\n",
    "        criterion = FocalLoss(alpha=alpha, gamma=2.0).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "        print(f\"Fold {fold}: n_train={len(df_tr):,}, n_test={len(df_te):,}, pos_frac={pos_frac:.4f}, focal_alpha={alpha:.4f}\")\n",
    "\n",
    "        history = []\n",
    "\n",
    "        for epoch in range(1, EPOCHS + 1):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for img, seq, lab in train_loader:\n",
    "                img = img.to(device, non_blocking=True)\n",
    "                seq = seq.to(device, non_blocking=True)\n",
    "                lab = lab.to(device, non_blocking=True)\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=use_amp):\n",
    "                    logits = model(img, seq)\n",
    "                    loss = criterion(logits, lab)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                total_loss += float(loss.item()) * lab.size(0)\n",
    "\n",
    "            # threshold on TRAIN only\n",
    "            y_tr, p_tr = predict_probs(model, train_eval_loader, device)\n",
    "            thr = best_threshold_f1(y_tr, p_tr)\n",
    "\n",
    "            # evaluate on fold test\n",
    "            y_true, y_prob = predict_probs(model, test_loader, device)\n",
    "            m05 = metrics_binary(y_true, y_prob, thr=0.5)\n",
    "            m = metrics_binary(y_true, y_prob, thr=thr)\n",
    "\n",
    "            pred_pos = float((y_prob >= thr).mean())\n",
    "            loss_epoch = total_loss / len(df_tr)\n",
    "\n",
    "            print(\n",
    "                f\"Fold {fold} | Epoch {epoch:02d} | \"\n",
    "                f\"loss={loss_epoch:.4f} | \"\n",
    "                f\"AUC={m['roc_auc']:.4f} | \"\n",
    "                f\"F1@0.5={m05['f1']:.4f} | F1@thr={m['f1']:.4f} | \"\n",
    "                f\"thr={thr:.3f} | pred_pos={pred_pos:.3f}\"\n",
    "            )\n",
    "\n",
    "            history.append({\n",
    "                \"fold\": fold,\n",
    "                \"epoch\": epoch,\n",
    "                \"loss\": loss_epoch,\n",
    "                \"auc\": m[\"roc_auc\"],\n",
    "                \"pr_auc\": m[\"pr_auc\"],\n",
    "                \"f1_0p5\": m05[\"f1\"],\n",
    "                \"f1_thr\": m[\"f1\"],\n",
    "                \"thr\": thr,\n",
    "                \"pred_pos\": pred_pos\n",
    "            })\n",
    "\n",
    "        # Save + plot fold training curves\n",
    "        hist_df = pd.DataFrame(history)\n",
    "        hist_csv = f\"history_fold{fold}.csv\"\n",
    "        hist_png = f\"Fold{fold}_TrainingCurves.png\"\n",
    "        hist_df.to_csv(hist_csv, index=False)\n",
    "        plot_fold_training_curves(hist_df, hist_png, fold=fold)\n",
    "        print(f\"Saved: {hist_csv}, {hist_png}\")\n",
    "\n",
    "        # Final fold metrics (use train-chosen thr)\n",
    "        y_tr, p_tr = predict_probs(model, train_eval_loader, device)\n",
    "        thr = best_threshold_f1(y_tr, p_tr)\n",
    "\n",
    "        y_true, y_prob = predict_probs(model, test_loader, device)\n",
    "        m05 = metrics_binary(y_true, y_prob, thr=0.5)\n",
    "        m = metrics_binary(y_true, y_prob, thr=thr)\n",
    "\n",
    "        # save predictions per fold (reproducibility)\n",
    "        np.savez_compressed(\n",
    "            f\"preds_fold{fold}.npz\",\n",
    "            y_true=y_true.astype(np.int8),\n",
    "            y_prob=y_prob.astype(np.float32),\n",
    "            thr=np.array([thr], dtype=np.float32),\n",
    "        )\n",
    "\n",
    "        # ROC/PR curves per fold\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "        roc_curves.append({\"fpr\": fpr, \"tpr\": tpr, \"auc\": m[\"roc_auc\"]})\n",
    "\n",
    "        prec, rec, _ = precision_recall_curve(y_true, y_prob)\n",
    "        pr_curves.append({\"precision\": prec, \"recall\": rec, \"ap\": m[\"pr_auc\"]})\n",
    "\n",
    "        # confusion matrix at fold thr\n",
    "        y_pred = (y_prob >= thr).astype(int)\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "        cm_sum += cm\n",
    "\n",
    "        m.update({\n",
    "            \"fold\": fold,\n",
    "            \"thr\": thr,\n",
    "            \"f1_0p5\": m05[\"f1\"],\n",
    "            \"recall_0p5\": m05[\"recall\"],\n",
    "            \"precision_0p5\": m05[\"precision\"],\n",
    "        })\n",
    "        fold_rows.append(m)\n",
    "\n",
    "        # per variant-type stratified\n",
    "        df_te2 = df_te.copy()\n",
    "        df_te2[\"y_prob\"] = y_prob\n",
    "        df_te2[\"group\"] = df_te2[\"MC\"].apply(tag_variant_group)\n",
    "\n",
    "        for g, sub in df_te2.groupby(\"group\"):\n",
    "            mm = metrics_binary(\n",
    "                sub[\"CLASS\"].values.astype(int),\n",
    "                sub[\"y_prob\"].values.astype(float),\n",
    "                thr=thr\n",
    "            )\n",
    "            mm[\"fold\"] = fold\n",
    "            mm[\"group\"] = g\n",
    "            per_type_rows.append(mm)\n",
    "\n",
    "        # free GPU memory between folds\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    fold_df = pd.DataFrame(fold_rows)\n",
    "    type_df = pd.DataFrame(per_type_rows)\n",
    "\n",
    "    print(\"\\n=== Gene-disjoint CV summary (mean ± std) ===\")\n",
    "    summary = fold_df.drop(columns=[\"fold\"]).agg([\"mean\", \"std\"]).T\n",
    "    print(summary)\n",
    "\n",
    "    print(\"\\n=== Variant-type stratified (mean over folds) ===\")\n",
    "    type_mean = (\n",
    "        type_df.groupby(\"group\")[[\"roc_auc\",\"pr_auc\",\"acc\",\"precision\",\"recall\",\"f1\",\"n\"]]\n",
    "        .mean()\n",
    "        .sort_values(\"n\", ascending=False)\n",
    "    )\n",
    "    print(type_mean)\n",
    "\n",
    "    fold_df.to_csv(SAVE_FOLD_CSV, index=False)\n",
    "    type_df.to_csv(SAVE_TYPE_CSV, index=False)\n",
    "    print(f\"\\nSaved: {SAVE_FOLD_CSV}\")\n",
    "    print(f\"Saved: {SAVE_TYPE_CSV}\")\n",
    "\n",
    "    # --------------------------\n",
    "    # Final paper plots (CV-level)\n",
    "    # --------------------------\n",
    "    plot_mean_roc(roc_curves, out_png=\"CV_ROC_mean.png\")\n",
    "    plot_mean_pr(pr_curves, prevalence=prevalence_all, out_png=\"CV_PR_mean.png\")\n",
    "    print(\"Saved: CV_ROC_mean.png, CV_PR_mean.png\")\n",
    "\n",
    "    plot_confmat(cm_sum, out_png=\"CV_ConfusionMatrix_sum.png\", normalize=False,\n",
    "                 title=\"Gene-disjoint CV - Confusion Matrix (sum over folds)\")\n",
    "    plot_confmat(cm_sum, out_png=\"CV_ConfusionMatrix_sum_norm.png\", normalize=True,\n",
    "                 title=\"Gene-disjoint CV - Confusion Matrix (row-normalized)\")\n",
    "    print(\"Saved: CV_ConfusionMatrix_sum.png, CV_ConfusionMatrix_sum_norm.png\")\n",
    "\n",
    "    plot_variant_type_bars(type_df, metric=\"f1\", out_png=\"VariantType_F1_bar.png\",\n",
    "                           title=\"Variant-type Stratified F1 (mean ± std over folds)\")\n",
    "    plot_variant_type_bars(type_df, metric=\"pr_auc\", out_png=\"VariantType_PRAUC_bar.png\",\n",
    "                           title=\"Variant-type Stratified PR-AUC (mean ± std over folds)\")\n",
    "    print(\"Saved: VariantType_F1_bar.png, VariantType_PRAUC_bar.png\")\n",
    "\n",
    "# Entry point (safe for .py script; in notebook you can just call main())\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f5a09-a3b3-49e9-94ff-cfe8dd747dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
